{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "chart_studio.tools.set_config_file(world_readable=True,\n",
    "                             sharing='public')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stonks = pd.read_csv(\"stonks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5032</th>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>151.5625</td>\n",
       "      <td>152.4843</td>\n",
       "      <td>149.6562</td>\n",
       "      <td>151.2187</td>\n",
       "      <td>6747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5031</th>\n",
       "      <td>2000-03-30</td>\n",
       "      <td>150.1562</td>\n",
       "      <td>151.9218</td>\n",
       "      <td>147.1250</td>\n",
       "      <td>148.6875</td>\n",
       "      <td>9491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5030</th>\n",
       "      <td>2000-03-31</td>\n",
       "      <td>149.6250</td>\n",
       "      <td>152.3125</td>\n",
       "      <td>148.4375</td>\n",
       "      <td>150.3750</td>\n",
       "      <td>9249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5029</th>\n",
       "      <td>2000-04-03</td>\n",
       "      <td>150.1250</td>\n",
       "      <td>151.2500</td>\n",
       "      <td>148.6875</td>\n",
       "      <td>151.2500</td>\n",
       "      <td>8508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>151.7500</td>\n",
       "      <td>153.0000</td>\n",
       "      <td>141.3906</td>\n",
       "      <td>150.1250</td>\n",
       "      <td>19585500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp      open      high       low     close    volume\n",
       "5032  2000-03-29  151.5625  152.4843  149.6562  151.2187   6747500\n",
       "5031  2000-03-30  150.1562  151.9218  147.1250  148.6875   9491900\n",
       "5030  2000-03-31  149.6250  152.3125  148.4375  150.3750   9249100\n",
       "5029  2000-04-03  150.1250  151.2500  148.6875  151.2500   8508200\n",
       "5028  2000-04-04  151.7500  153.0000  141.3906  150.1250  19585500"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stonks = stonks.iloc[::-1]\n",
    "stonks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30754801],\n",
       "       [0.29818118],\n",
       "       [0.30442586],\n",
       "       ...,\n",
       "       [0.43437072],\n",
       "       [0.43999556],\n",
       "       [0.44458424]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_percent = 0.7\n",
    "val_percent = 0.1\n",
    "\n",
    "close_data = stonks[[\"open\", \"high\", \"low\", \"close\", \"close\"]].values.reshape((-1,5))\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(close_data)\n",
    "\n",
    "close_data = scaler.transform(close_data)\n",
    "split = int(split_percent*len(close_data))\n",
    "val = int((val_percent + split_percent) * len(close_data))\n",
    "close_train = close_data[:split]\n",
    "close_val = close_data[split:val]\n",
    "close_test = close_data[val:]\n",
    "\n",
    "date_train = stonks.timestamp[:split]\n",
    "date_val = stonks.timestamp[split:val]\n",
    "date_test = stonks.timestamp[val:]\n",
    "\n",
    "close_train[0:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 176 steps, validate for 25 steps\n",
      "Epoch 1/92\n",
      "176/176 [==============================] - 2s 14ms/step - loss: 0.0026 - accuracy: 0.1970 - val_loss: 7.0307e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 5.3889e-04 - accuracy: 0.2124 - val_loss: 1.8375e-04 - val_accuracy: 0.4576\n",
      "Epoch 3/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 4.7037e-04 - accuracy: 0.2210 - val_loss: 2.0074e-04 - val_accuracy: 0.4576\n",
      "Epoch 4/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 3.6491e-04 - accuracy: 0.2261 - val_loss: 0.0020 - val_accuracy: 0.3147\n",
      "Epoch 5/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 3.2501e-04 - accuracy: 0.2318 - val_loss: 8.4932e-04 - val_accuracy: 0.0414\n",
      "Epoch 6/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 2.7668e-04 - accuracy: 0.2192 - val_loss: 1.7163e-04 - val_accuracy: 0.0352\n",
      "Epoch 7/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 2.7117e-04 - accuracy: 0.2287 - val_loss: 1.0442e-04 - val_accuracy: 0.4576\n",
      "Epoch 8/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.9873e-04 - accuracy: 0.2409 - val_loss: 3.3726e-04 - val_accuracy: 0.1863\n",
      "Epoch 9/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.9216e-04 - accuracy: 0.2384 - val_loss: 1.9321e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.7410e-04 - accuracy: 0.2384 - val_loss: 4.6425e-04 - val_accuracy: 0.1863\n",
      "Epoch 11/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.8253e-04 - accuracy: 0.2230 - val_loss: 3.0697e-04 - val_accuracy: 0.4555\n",
      "Epoch 12/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.7422e-04 - accuracy: 0.2418 - val_loss: 7.9591e-05 - val_accuracy: 0.4576\n",
      "Epoch 13/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.7204e-04 - accuracy: 0.2387 - val_loss: 9.9041e-04 - val_accuracy: 0.3043\n",
      "Epoch 14/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.3633e-04 - accuracy: 0.2509 - val_loss: 8.7265e-05 - val_accuracy: 0.0290\n",
      "Epoch 15/92\n",
      "176/176 [==============================] - 2s 13ms/step - loss: 1.3783e-04 - accuracy: 0.2526 - val_loss: 2.7728e-04 - val_accuracy: 0.3209\n",
      "Epoch 16/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.4187e-04 - accuracy: 0.2409 - val_loss: 1.6668e-04 - val_accuracy: 0.1594\n",
      "Epoch 17/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.8488e-04 - accuracy: 0.2652 - val_loss: 7.3193e-05 - val_accuracy: 0.4472\n",
      "Epoch 18/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.1255e-04 - accuracy: 0.2504 - val_loss: 7.5319e-05 - val_accuracy: 0.4576\n",
      "Epoch 19/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.1252e-04 - accuracy: 0.2481 - val_loss: 3.2140e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.1940e-04 - accuracy: 0.2578 - val_loss: 1.7584e-04 - val_accuracy: 0.1884\n",
      "Epoch 21/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.6450e-04 - accuracy: 0.2509 - val_loss: 9.6696e-05 - val_accuracy: 0.1863\n",
      "Epoch 22/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.1387e-04 - accuracy: 0.2538 - val_loss: 1.1903e-04 - val_accuracy: 0.4037\n",
      "Epoch 23/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0926e-04 - accuracy: 0.2661 - val_loss: 8.9766e-05 - val_accuracy: 0.2588\n",
      "Epoch 24/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1765e-04 - accuracy: 0.2692 - val_loss: 2.3882e-04 - val_accuracy: 0.3106\n",
      "Epoch 25/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.3259e-04 - accuracy: 0.2743 - val_loss: 9.0142e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.3852e-04 - accuracy: 0.2789 - val_loss: 8.2957e-04 - val_accuracy: 0.4576\n",
      "Epoch 27/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0625e-04 - accuracy: 0.2758 - val_loss: 6.1200e-05 - val_accuracy: 0.1863\n",
      "Epoch 28/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0535e-04 - accuracy: 0.2812 - val_loss: 6.7569e-05 - val_accuracy: 0.4555\n",
      "Epoch 29/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0239e-04 - accuracy: 0.2969 - val_loss: 5.9412e-05 - val_accuracy: 0.4576\n",
      "Epoch 30/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1424e-04 - accuracy: 0.2775 - val_loss: 9.8373e-05 - val_accuracy: 0.0062\n",
      "Epoch 31/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0008e-04 - accuracy: 0.2972 - val_loss: 3.6607e-04 - val_accuracy: 0.4576\n",
      "Epoch 32/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.4485e-04 - accuracy: 0.2906 - val_loss: 4.0304e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0282e-04 - accuracy: 0.3012 - val_loss: 9.3743e-05 - val_accuracy: 0.4555\n",
      "Epoch 34/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1128e-04 - accuracy: 0.3114 - val_loss: 9.3117e-04 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1558e-04 - accuracy: 0.2926 - val_loss: 8.2442e-05 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0233e-04 - accuracy: 0.3094 - val_loss: 1.2206e-04 - val_accuracy: 0.4513\n",
      "Epoch 37/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.9515e-05 - accuracy: 0.2932 - val_loss: 5.7275e-05 - val_accuracy: 0.1615\n",
      "Epoch 38/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0161e-04 - accuracy: 0.3066 - val_loss: 1.0350e-04 - val_accuracy: 0.4576\n",
      "Epoch 39/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0362e-04 - accuracy: 0.3317 - val_loss: 8.3141e-05 - val_accuracy: 0.4576\n",
      "Epoch 40/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.2549e-04 - accuracy: 0.3246 - val_loss: 5.8001e-05 - val_accuracy: 0.3106\n",
      "Epoch 41/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.2996e-05 - accuracy: 0.3077 - val_loss: 1.8006e-04 - val_accuracy: 0.4472\n",
      "Epoch 42/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.5001e-05 - accuracy: 0.3380 - val_loss: 4.9837e-05 - val_accuracy: 0.1863\n",
      "Epoch 43/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1510e-04 - accuracy: 0.3351 - val_loss: 9.3629e-05 - val_accuracy: 0.1863\n",
      "Epoch 44/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 2.2962e-04 - accuracy: 0.3146 - val_loss: 2.3121e-04 - val_accuracy: 0.4576\n",
      "Epoch 45/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1098e-04 - accuracy: 0.3217 - val_loss: 7.6662e-05 - val_accuracy: 0.4576\n",
      "Epoch 46/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0627e-04 - accuracy: 0.3334 - val_loss: 7.6441e-05 - val_accuracy: 0.4576\n",
      "Epoch 47/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1443e-04 - accuracy: 0.3431 - val_loss: 7.5636e-05 - val_accuracy: 0.4555\n",
      "Epoch 48/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.2530e-05 - accuracy: 0.3340 - val_loss: 1.2062e-04 - val_accuracy: 0.4576\n",
      "Epoch 49/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0214e-04 - accuracy: 0.3411 - val_loss: 4.6469e-05 - val_accuracy: 0.4576\n",
      "Epoch 50/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.3073e-04 - accuracy: 0.3426 - val_loss: 3.3821e-04 - val_accuracy: 0.4576\n",
      "Epoch 51/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1001e-04 - accuracy: 0.3463 - val_loss: 4.4223e-04 - val_accuracy: 0.1863\n",
      "Epoch 52/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.2257e-04 - accuracy: 0.3506 - val_loss: 4.5641e-04 - val_accuracy: 0.4576\n",
      "Epoch 53/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1137e-04 - accuracy: 0.3660 - val_loss: 5.7129e-05 - val_accuracy: 0.0021\n",
      "Epoch 54/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0259e-04 - accuracy: 0.3620 - val_loss: 7.5223e-04 - val_accuracy: 0.1863\n",
      "Epoch 55/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.2539e-04 - accuracy: 0.3634 - val_loss: 4.4266e-05 - val_accuracy: 0.4576\n",
      "Epoch 56/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.6228e-05 - accuracy: 0.3645 - val_loss: 1.7506e-04 - val_accuracy: 0.2360\n",
      "Epoch 57/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0137e-04 - accuracy: 0.3488 - val_loss: 7.6802e-05 - val_accuracy: 0.4576\n",
      "Epoch 58/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.7588e-05 - accuracy: 0.3591 - val_loss: 5.5386e-05 - val_accuracy: 0.2899\n",
      "Epoch 59/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.5356e-05 - accuracy: 0.3757 - val_loss: 6.8385e-05 - val_accuracy: 0.3147\n",
      "Epoch 60/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.9718e-05 - accuracy: 0.3334 - val_loss: 5.8809e-05 - val_accuracy: 0.4576\n",
      "Epoch 61/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.3506e-04 - accuracy: 0.3705 - val_loss: 3.2532e-04 - val_accuracy: 0.4576\n",
      "Epoch 62/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.2778e-04 - accuracy: 0.3794 - val_loss: 6.1515e-05 - val_accuracy: 0.3602\n",
      "Epoch 63/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1380e-04 - accuracy: 0.3691 - val_loss: 5.6282e-05 - val_accuracy: 0.4576\n",
      "Epoch 64/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1694e-04 - accuracy: 0.3800 - val_loss: 9.5827e-05 - val_accuracy: 0.4576\n",
      "Epoch 65/92\n",
      "176/176 [==============================] - 3s 15ms/step - loss: 1.1653e-04 - accuracy: 0.3617 - val_loss: 1.1984e-04 - val_accuracy: 0.4576\n",
      "Epoch 66/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1453e-04 - accuracy: 0.3734 - val_loss: 5.4870e-05 - val_accuracy: 0.2298\n",
      "Epoch 67/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0155e-04 - accuracy: 0.3780 - val_loss: 1.0537e-04 - val_accuracy: 0.4576\n",
      "Epoch 68/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 9.9217e-05 - accuracy: 0.3703 - val_loss: 1.1437e-04 - val_accuracy: 0.2443\n",
      "Epoch 69/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0388e-04 - accuracy: 0.3808 - val_loss: 9.1204e-05 - val_accuracy: 0.4576\n",
      "Epoch 70/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.0117e-04 - accuracy: 0.3802 - val_loss: 1.5993e-04 - val_accuracy: 0.4576\n",
      "Epoch 71/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.0346e-04 - accuracy: 0.3768 - val_loss: 4.1359e-05 - val_accuracy: 0.4576\n",
      "Epoch 72/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.0733e-04 - accuracy: 0.3651 - val_loss: 4.2836e-05 - val_accuracy: 0.1863\n",
      "Epoch 73/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 9.4258e-05 - accuracy: 0.3757 - val_loss: 4.2440e-05 - val_accuracy: 0.3644\n",
      "Epoch 74/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.0703e-04 - accuracy: 0.3680 - val_loss: 1.7339e-04 - val_accuracy: 0.4576\n",
      "Epoch 75/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.1248e-04 - accuracy: 0.3720 - val_loss: 0.0013 - val_accuracy: 0.4576\n",
      "Epoch 76/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.1634e-04 - accuracy: 0.3705 - val_loss: 6.9437e-05 - val_accuracy: 0.4576\n",
      "Epoch 77/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.0395e-04 - accuracy: 0.3740 - val_loss: 7.1841e-05 - val_accuracy: 0.4576\n",
      "Epoch 78/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.1116e-04 - accuracy: 0.3905 - val_loss: 8.8318e-05 - val_accuracy: 0.1863\n",
      "Epoch 79/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.0453e-04 - accuracy: 0.3711 - val_loss: 9.5234e-05 - val_accuracy: 0.2712\n",
      "Epoch 80/92\n",
      "176/176 [==============================] - 2s 10ms/step - loss: 1.1221e-04 - accuracy: 0.3614 - val_loss: 1.5313e-04 - val_accuracy: 0.4576\n",
      "Epoch 81/92\n",
      "176/176 [==============================] - 2s 9ms/step - loss: 9.6158e-05 - accuracy: 0.3842 - val_loss: 8.2303e-05 - val_accuracy: 0.1884\n",
      "Epoch 82/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.5003e-05 - accuracy: 0.3840 - val_loss: 4.0929e-05 - val_accuracy: 0.4431\n",
      "Epoch 83/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.0303e-05 - accuracy: 0.3751 - val_loss: 1.8095e-04 - val_accuracy: 0.1863\n",
      "Epoch 84/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0487e-04 - accuracy: 0.3631 - val_loss: 9.3736e-05 - val_accuracy: 0.4576\n",
      "Epoch 85/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0724e-04 - accuracy: 0.3705 - val_loss: 4.2364e-05 - val_accuracy: 0.4576\n",
      "Epoch 86/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.3032e-04 - accuracy: 0.3566 - val_loss: 4.4650e-05 - val_accuracy: 0.4576\n",
      "Epoch 87/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.5070e-05 - accuracy: 0.3717 - val_loss: 4.3831e-05 - val_accuracy: 0.2277\n",
      "Epoch 88/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.0261e-04 - accuracy: 0.3934 - val_loss: 1.1564e-04 - val_accuracy: 0.1863\n",
      "Epoch 89/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 1.1366e-04 - accuracy: 0.3754 - val_loss: 8.6710e-05 - val_accuracy: 0.4576\n",
      "Epoch 90/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0044e-04 - accuracy: 0.3734 - val_loss: 1.1920e-04 - val_accuracy: 0.4327\n",
      "Epoch 91/92\n",
      "176/176 [==============================] - 2s 12ms/step - loss: 9.2694e-05 - accuracy: 0.3557 - val_loss: 6.8370e-05 - val_accuracy: 0.4576\n",
      "Epoch 92/92\n",
      "176/176 [==============================] - 2s 11ms/step - loss: 1.0365e-04 - accuracy: 0.3668 - val_loss: 4.1034e-05 - val_accuracy: 0.4576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe60e748f60>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back = 20\n",
    "num_epochs = 92\n",
    "\n",
    "train_generator = keras.preprocessing.sequence.TimeseriesGenerator(close_train, close_train, length=look_back, batch_size=20)   \n",
    "val_generator = keras.preprocessing.sequence.TimeseriesGenerator(close_val, close_val, length=look_back, batch_size=20)     \n",
    "test_generator = keras.preprocessing.sequence.TimeseriesGenerator(close_test, close_test, length=look_back, batch_size=1)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.LSTM(256,\n",
    "                      activation=\"relu\",\n",
    "                      #recurrent_activation=\"sigmoid\",\n",
    "                      use_bias=True,\n",
    "                      input_shape=(look_back, 5)),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(5),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(train_generator, epochs=num_epochs, verbose=1, validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9219494742195415\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(test_generator)\n",
    "\n",
    "def denorm(d): \n",
    "    return scaler.inverse_transform(d)[0:, 4:].reshape((-1))\n",
    "close_train_1 = denorm(close_train)\n",
    "close_val_1 = denorm(close_val)\n",
    "close_test_1 = denorm(close_test)\n",
    "prediction_1 = denorm(prediction)\n",
    "\n",
    "diff = close_test_1[look_back:] - prediction_1\n",
    "avg_diff = np.mean(diff)\n",
    "print(avg_diff)\n",
    "\n",
    "def predict(num_prediction, before_lookback, model):\n",
    "    if (before_lookback > 0):\n",
    "        prediction_list = close_data[-look_back - before_lookback:-before_lookback]\n",
    "    else:\n",
    "        prediction_list = close_data[-look_back:]\n",
    "    for _ in range(num_prediction):\n",
    "        x = prediction_list[-look_back:]\n",
    "#         print(\"Pred list\", prediction_list)\n",
    "        x = x.reshape((1, look_back, 5))\n",
    "        out = model.predict(x)\n",
    "#         print(\"Out\", out)\n",
    "        prediction_list = np.append(prediction_list, out, axis=0)\n",
    "    prediction_list = prediction_list[look_back:]\n",
    "        \n",
    "    return prediction_list\n",
    "    \n",
    "def predict_dates(num_prediction, before_lookback):\n",
    "    last_date = stonks.timestamp.values[-1 - before_lookback]\n",
    "    prediction_dates = pd.date_range(last_date, periods=num_prediction+1+before_lookback).tolist()\n",
    "    return prediction_dates\n",
    "\n",
    "num_prediction = 4\n",
    "before_lookback = 0\n",
    "forecast = denorm(predict(num_prediction, before_lookback, model))\n",
    "forecast_dates = predict_dates(num_prediction, before_lookback)\n",
    "prediction_1 = np.concatenate([[None] * look_back, prediction_1 + avg_diff])\n",
    "trace1 = go.Scatter(\n",
    "    x = np.concatenate([date_train, date_val]),\n",
    "    y = np.concatenate([close_train_1, close_val_1]),\n",
    "    mode = 'lines',\n",
    "    name = 'Real Data - trained'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = date_test,\n",
    "    y = prediction_1,\n",
    "    mode = 'lines',\n",
    "    name = 'Prediction'\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x = date_test,\n",
    "    y = close_test_1,\n",
    "    mode='lines',\n",
    "    name = 'Real Data - untrained'\n",
    ")\n",
    "forecast = go.Scatter(\n",
    "    x = forecast_dates,\n",
    "    y = forecast + avg_diff,\n",
    "    mode = 'lines+markers+text',\n",
    "    texttemplate = \"%{y:.2f}\",\n",
    "    name = 'Forecast'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"SPY\",\n",
    "    xaxis = {'title' : \"Date\"},\n",
    "    yaxis = {'title' : \"Close\"}\n",
    ")\n",
    "fig = go.Figure(data=[trace1, trace2, trace3, forecast], layout=layout)\n",
    "fig.show(renderer=\"browser\")\n",
    "# py.plot(fig, filename = 'basic-line', renderer=\"browser\", auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.write_html(fig, file='tracing_pred.html', auto_open=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('good_model_bak_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"good_model_bak_2.h5\", custom_objects={'loss': customloss()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
